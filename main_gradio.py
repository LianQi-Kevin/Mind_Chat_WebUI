import argparse
import logging
import os
import time
from typing import List

import gradio as gr
from modelscope.hub.snapshot_download import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig

from tools.logging_utils import log_set


def load_model(model_path: str = "X-D-Lab/MindChat-Qwen-7B"):
    logging.debug(f"Starting Loading {os.path.basename(model_path)}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True,
                                                 fp16=True).eval()
    model.generation_config = GenerationConfig.from_pretrained(model_path, trust_remote_code=True)
    logging.info(f"Loading {os.path.basename(model_path)} finished")
    return tokenizer, model


class MindChat(object):
    def __init__(self, model_path: str = None, cache_dir: str = "./", model_id: str = "X-D-Lab/MindChat-Qwen-1_8B"):
        self.model_path = snapshot_download(model_id=model_id, cache_dir=cache_dir,
                                            revision='v1.0.0') if model_path is None else model_path
        self.tokenizer, self.model = load_model(model_path)

    def get_response(self, message: str, history: List[List[str]]) -> str:
        logging.debug(f"Start a chat, message: '{message}', history: '{history}'")
        history = history[-20:]  # todo: æš‚æ—¶é‡‡ç”¨[-20:]é˜²æ­¢tokenè¶…é™, å¾…æ›´æ¢ä¸ºtokenizer
        response, _ = self.model.chat(
            tokenizer=self.tokenizer,
            query=message,
            history=[] if history is None else history,
        )
        logging.debug(f"Finish a chat, response: '{response}'")
        return response

    def chat(self, message: str = "what can you doï¼Ÿ", history: List[List[str]] = None):
        history = [] if history is None else history
        response = self.get_response(message=message, history=history)
        history.append([message, ""])
        for character in response:
            history[-1][1] += character
            time.sleep(0.05)
            yield history

    def retry(self, history: List[List[str]]):
        """ä»historyåˆ—è¡¨ä¸­é‡æ–°æ‰§è¡Œæœ€åä¸€æ¬¡å¯¹è¯"""
        logging.debug(f"Retry history: {history}")
        if history:
            message = history[-1][0]
            history = history[:-1]
            response = self.get_response(message=message, history=history)
            history.append([message, ""])
            for character in response:
                history[-1][1] += character
                time.sleep(0.05)
                yield history
        else:
            return []

    @staticmethod
    def undo(history: List[List[str]]) -> List[List[str]]:
        """ä»historyåˆ—è¡¨ä¸­åˆ é™¤æœ€åä¸€æ¬¡å¯¹è¯"""
        return history[:-1]


def main(mind_chat: MindChat):
    with gr.Blocks(title="ğŸ‹MindChat: æ¼«è°ˆå¿ƒç†å¤§æ¨¡å‹") as demo:
        with gr.Column():
            gr.Markdown(
                """
                # ğŸ‹MindChat: æ¼«è°ˆå¿ƒç†å¤§æ¨¡å‹
                
                ğŸ” MindChat(æ¼«è°ˆ): æ—¨åœ¨é€šè¿‡è¥é€ è½»æ¾ã€å¼€æ”¾çš„äº¤è°ˆç¯å¢ƒ, ä»¥æ”¾æ¾èº«å¿ƒã€äº¤æµæ„Ÿå—æˆ–åˆ†äº«ç»éªŒçš„æ–¹å¼, ä¸ºç”¨æˆ·æä¾›éšç§ã€æ¸©æš–ã€å®‰å…¨ã€åŠæ—¶ã€æ–¹ä¾¿çš„å¯¹è¯ç¯å¢ƒ, ä»è€Œå¸®åŠ©ç”¨æˆ·å…‹æœå„ç§å›°éš¾å’ŒæŒ‘æˆ˜, å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•ã€‚
            
                ğŸ¦Š æ— è®ºæ˜¯åœ¨å·¥ä½œåœºæ‰€è¿˜æ˜¯åœ¨ä¸ªäººç”Ÿæ´»ä¸­, MindChatæœŸæœ›é€šè¿‡è‡ªèº«çš„åŠªåŠ›å’Œä¸“ä¸šçŸ¥è¯†, åœ¨ä¸¥æ ¼ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹, å…¨æ—¶æ®µå…¨å¤©å€™ä¸ºç”¨æˆ·æä¾›å…¨é¢çš„å¿ƒç†é™ªä¼´å’Œå€¾å¬, åŒæ—¶å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•, ä»¥æœŸä¸ºå»ºè®¾ä¸€ä¸ªæ›´åŠ å¥åº·ã€åŒ…å®¹å’Œå¹³ç­‰çš„ç¤¾ä¼šè´¡çŒ®åŠ›é‡ã€‚
                
                ğŸ™…â€ ç›®å‰ï¼ŒMindChatè¿˜ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„å¿ƒç†åŒ»ç”Ÿå’Œå¿ƒç†å’¨è¯¢å¸ˆï¼Œæ— æ³•åšå‡ºä¸“ä¸šçš„å¿ƒç†è¯Šæ–­æŠ¥å‘Šã€‚è™½MindChatåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æè‡´æ³¨é‡æ¨¡å‹å®‰å…¨å’Œä»·å€¼è§‚æ­£å‘å¼•å¯¼ï¼Œä½†ä»æ— æ³•ä¿è¯æ¨¡å‹è¾“å‡ºæ­£ç¡®ä¸”æ— å®³ï¼Œå†…å®¹ä¸Šæ¨¡å‹ä½œè€…åŠå¹³å°ä¸æ‰¿æ‹…ç›¸å…³è´£ä»»ã€‚
                
                ğŸ‘ æ›´ä¸ºä¼˜è´¨ã€å®‰å…¨ã€æ¸©æš–çš„æ¨¡å‹æ­£åœ¨èµ¶æ¥çš„è·¯ä¸Šï¼Œæ¬¢è¿å…³æ³¨ï¼š[MindChat Github](https://github.com/X-D-Lab/MindChat)
                """
            )
            chatbot = gr.Chatbot([], elem_id="chatBot", bubble_full_width=False)
            with gr.Row():
                message_text = gr.Textbox(placeholder="Type a message...", lines=1, max_lines=10, interactive=True,
                                          scale=6, show_label=False)
                submit_btn = gr.Button(value="å‘é€", show_label=False, elem_classes="submitBtn", scale=2)
            with gr.Row():
                retry_btn = gr.Button(value="ğŸ”„ é‡æ–°ç”Ÿæˆ", show_label=False, elem_classes="retryBtn")
                undo_btn = gr.Button(value="â†©ï¸ æ’¤é”€", show_label=False, elem_classes="undoBtn")
                clear_btn = gr.Button(value="ğŸ—‘ï¸ æ¸…é™¤å†å²", show_label=False, elem_classes="clearBtn")

        message_text.submit(mind_chat.chat, inputs=[message_text, chatbot], outputs=[chatbot])
        submit_btn.click(mind_chat.chat, inputs=[message_text, chatbot], outputs=[chatbot])
        retry_btn.click(mind_chat.retry, inputs=[chatbot], outputs=[chatbot])
        undo_btn.click(mind_chat.undo, inputs=[chatbot], outputs=[chatbot])
        clear_btn.click(lambda *args: [], inputs=[chatbot], outputs=[chatbot])
    return demo


if __name__ == '__main__':
    # Args
    parser = argparse.ArgumentParser("MindChat Gradio WebUI")
    parser.add_argument("--model_path", "-m", type=str, help="model path", nargs='?', default=None)
    parser.add_argument("--model_id", "-mid", type=str, help="modelscope model id", nargs='?',
                        default="X-D-Lab/MindChat-Qwen-1_8B")
    parser.add_argument("--cache_dir", "-c", type=str, help="cache dir", nargs='?', default="./")
    parser.add_argument("--port", "-p", type=int, help="server port", nargs='?', default=6006)
    args_ = parser.parse_args()

    # logging
    log_set(log_level=logging.INFO, log_save=True)

    # Gradio Page
    demo_ = main(mind_chat=MindChat(model_path=args_.model_path, model_id=args_.model_id, cache_dir=args_.cache_dir))

    demo_.queue(concurrency_count=2, max_size=15, status_update_rate="auto")
    demo_.launch(show_error=False, share=False, quiet=True, server_port=args_.port, debug=False, show_api=False)
