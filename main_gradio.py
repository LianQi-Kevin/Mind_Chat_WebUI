import logging
import os

import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig

from tools.logging_utils import log_set


def get_model(model_path: str = "X-D-Lab/MindChat-Qwen-7B"):
    logging.info(f"Starting Loading {os.path.basename(model_path)}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True,
                                                 fp16=True).eval()
    model.generation_config = GenerationConfig.from_pretrained(model_path, trust_remote_code=True)
    logging.info(f"Loading {os.path.basename(model_path)} finished")
    return tokenizer, model


def common_demo(model_path: str):
    tokenizer, model = get_model(model_path=model_path)
    history = []

    while True:
        history = history[-20:]
        message = input("User: ")
        response, history = model.chat(tokenizer=tokenizer, query=message, history=history,
                                       system="You are a helpful assistant.")
        history.append((message, response))
        print(f"MindChat: {response}")


def main(model_path: str):
    logging.debug(f"Into Main, model_path: '{model_path}'")
    # tokenizer, model = get_model(model_path)
    with gr.Blocks(title="ğŸ‹MindChat: æ¼«è°ˆå¿ƒç†å¤§æ¨¡å‹", css="MindChat.css") as demo:
        with gr.Column():
            gr.Markdown(
                """
                ğŸ” MindChat(æ¼«è°ˆ): æ—¨åœ¨é€šè¿‡è¥é€ è½»æ¾ã€å¼€æ”¾çš„äº¤è°ˆç¯å¢ƒ, ä»¥æ”¾æ¾èº«å¿ƒã€äº¤æµæ„Ÿå—æˆ–åˆ†äº«ç»éªŒçš„æ–¹å¼, ä¸ºç”¨æˆ·æä¾›éšç§ã€æ¸©æš–ã€å®‰å…¨ã€åŠæ—¶ã€æ–¹ä¾¿çš„å¯¹è¯ç¯å¢ƒ, ä»è€Œå¸®åŠ©ç”¨æˆ·å…‹æœå„ç§å›°éš¾å’ŒæŒ‘æˆ˜, å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•.
                
                ğŸ¦Š æ— è®ºæ˜¯åœ¨å·¥ä½œåœºæ‰€è¿˜æ˜¯åœ¨ä¸ªäººç”Ÿæ´»ä¸­, MindChatæœŸæœ›é€šè¿‡è‡ªèº«çš„åŠªåŠ›å’Œä¸“ä¸šçŸ¥è¯†, åœ¨ä¸¥æ ¼ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹, å…¨æ—¶æ®µå…¨å¤©å€™ä¸ºç”¨æˆ·æä¾›å…¨é¢çš„å¿ƒç†é™ªä¼´å’Œå€¾å¬, åŒæ—¶å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•, ä»¥æœŸä¸ºå»ºè®¾ä¸€ä¸ªæ›´åŠ å¥åº·ã€åŒ…å®¹å’Œå¹³ç­‰çš„ç¤¾ä¼šè´¡çŒ®åŠ›é‡.
                
                ğŸ™…â€ ç›®å‰ï¼ŒMindChatè¿˜ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„å¿ƒç†åŒ»ç”Ÿå’Œå¿ƒç†å’¨è¯¢å¸ˆï¼Œæ— æ³•åšå‡ºä¸“ä¸šçš„å¿ƒç†è¯Šæ–­æŠ¥å‘Šã€‚è™½MindChatåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æè‡´æ³¨é‡æ¨¡å‹å®‰å…¨å’Œä»·å€¼è§‚æ­£å‘å¼•å¯¼ï¼Œä½†ä»æ— æ³•ä¿è¯æ¨¡å‹è¾“å‡ºæ­£ç¡®ä¸”æ— å®³ï¼Œå†…å®¹ä¸Šæ¨¡å‹ä½œè€…åŠå¹³å°ä¸æ‰¿æ‹…ç›¸å…³è´£ä»»ã€‚
                
                ğŸ‘ æ›´ä¸ºä¼˜è´¨ã€å®‰å…¨ã€æ¸©æš–çš„æ¨¡å‹æ­£åœ¨èµ¶æ¥çš„è·¯ä¸Šï¼Œæ¬¢è¿å…³æ³¨ï¼š[MindChat Github](https://github.com/X-D-Lab/MindChat)
                """
            )
            gr.Chatbot()
            with gr.Row():
                translated_text = gr.Textbox(label="Target Text", lines=3, max_lines=10, interactive=True)
                submit_btn = gr.Button(value="å‘é€", show_label=False, elem_classes="submit_btn")
            with gr.Row():
                retry_btn = gr.Button(value="ğŸ”„ é‡æ–°ç”Ÿæˆ", show_label=False, elem_classes="retry_btn")
                undo_btn = gr.Button(value="â†©ï¸ æ’¤é”€", show_label=False, elem_classes="undo_btn")
                clear_btn = gr.Button(value="ğŸ—‘ï¸ æ¸…é™¤å†å²", show_label=False, elem_classes="clear_btn")
    return demo


if __name__ == '__main__':
    log_set(log_level=logging.DEBUG, log_save=True)
    demo_ = main(model_path="/root/autodl-tmp/models/MindChat-Qwen-1_8B")

    demo_.queue(concurrency_count=2, max_size=15, status_update_rate="auto")
    demo_.launch(show_error=False, share=False, quiet=False, server_port=6006)
